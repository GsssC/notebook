## 树莓派如何关机
adb reboot -p
## android adb 重载系统根目录为读写
mount rw,remount rootfs /
## linux
- **/var/run目录下pid文件的作用**:linux系统中/var/run/目录下的*.pid文件是一个文本文件，其内容只有一行，即某个进程的PID。.pid文件的作用是防止进程启动多个副本，只有获得特定pid文件（固定路径和文件名）的写入权限（F_WRLCK）的进程才能正常启动并将自身的进程PID写入该文件，其它同一程序的多余进程则自动退出。
- 临时修改hostname，重启后失效`hostname node1`，将hostname改为node1。 	
## oci
[open container initiative](https://xuanwo.io/2019/08/06/oci-intro/)

![images](images/oci.png)

![images](images/cri-o.png)
## APUE(advanced programming in the unix environment)
## gcc
- 默认预处理，编译，汇编，链接至最后的可执行文件。
- -c 生成.o(obj文档)
- -S 生成.s(汇编代码)
- -E 生成预处理后的文档，没有具体后缀
- -o 设定输出
## vim 
- ctrl+o 向后跳转
- ctrl+i 向前跳转
- shift+r 进入替换模式
## 工作目录
每个进程都有一个工作目录，有时称为当前工作目录。所有相对路径名都是从工作目录开始解释。进程可以用chdir函数更改其工作目录。
## 作业
- 分布式：基于raft协议的分布式缓存
- 大数据：
	- 读书报告（基于raft协议的分布式缓存）
	- sougou数据分析
- PaaS: 基于k8s的安卓集群应用控制。
## go包
- spf/cobra：CLI命令行库
## adb运行流程
![images](images/adb.png)
## android 沙箱
Android applications run on a VM (Virtual Machine), and are completely isolated one from another due to the permissions Android gives each app. Basically each application on android is a separate user, and they have their own space on the "disk". That means Applications cannot access each other's space, Cannot uninstall or mess with each others data.

Only one user has access to OS and all applications, it's the root. So when people "root" their phone, they basically get root permissions, and can mess with every application including system services and libraries. But that's off topic

Android provides layer of protection in that it doesn’t give one app access to the resource of another app. This is known as the ‘sandbox’ where every app gets to play in its own sandbox and can’t use another app’s toys! Android does this by giving each app a unique user id (a UID) and by running that app as a separate process with that UID. Only processes with the same UIDs can share resources which, as each ID is uniquely assigned, means that no other apps have permission.
## android 进程树
- init
- zygote
- system_server:dalvik(system_servers:java)
	- package manager service(app的安装与卸载)
	- activity manager service(app的启动与停止)
	- ...
## markdown
`-`只能用来做列表，不好作为小标题

markdown中为了兼容HTML,尖括号无法由`\`转义输出，应通过`&lt;文字&gt;`输出尖括号:&lt;文字&gt;
## raft
#### 竞选机制  
![images](images/raft_election.png)
![images](images/raft_term.png)  
raft采用心跳机制触发leader竞选。当servers启动时，他们都处于followers状态，且只要他们持续收到从leader或candidate发来的有效RPCs请求，就会持续保持followers状态。leader向followers发送周期性心跳(AppendEntries RPCs that carry no log entries)以维持其权威。如果一个follower在一个段时间内没有接受到任何RPCs请求(称为election timeout)，就会认为当前没有存活leader并开始试图竞选称为leader: 
1. 当前周期(term)加一
1. 将节点状态转变为candidate
1. 向集群中的其他servers并行发送RequestVote RPCs  

一个candidate保持其candidate状态直到下列三个事情发生:
1. 赢得选举，转向leader状态
1. 另一个candidate赢得选举并接收到其发送的心跳包，返回follower状态
1. 一周期时间内没有人赢得选举。

若candidate在一个term内赢得集群中大部分选票，则其赢得选举。follower根据谁先来给先给谁投票的原则，一个term内其只投出一张选票给最先发来请求的candidate。大多数原则保证了在一个term内仅有一个leader产生。leader产生后向集群中的其他server发送心跳包建立权威和阻止新的竞选。

follower由于收到leader的心跳包而转变成candidate参与竞选，等待选票时，可能又会收到来自旧leader，或者在断开与leader连接时集群中新leader的心跳包。
1. 来自旧leader的心跳包(**candidate_term>心跳包_term**)，由于此时candidate的term大于旧leader的term，此心跳包被丢弃。
1. 来自同一term的leader的心跳包，即其竞选失败(**candidate_term=心跳包_term**)，被其他candidtae抢先，将状态转为follower。
1. 来自新term的心跳包包(**candidate_term&lt;心跳包_term**)，将状态转为follower。

## EdgeD
#### 概览
Edged是edge节点管理pod生命周期的模块。其帮助用户在edge节点部署容器化的工作负载或者应用。这里的工作负载可以表现为简单的遥测数据操作。在云端使用`kubectl`命令，用户可以发送命令启用这些工作负载。

现在支持Docker作为容器和镜像管理的运行时。未来也会加入其他容器运行时，比如 containerd。

EdgeD中为了实现其功能，有着如下诸多模块在运行。
#### Pod Management
Pod Management负责添加，删除和修改pod。其还通过pod status manager和pleg(pod lifecycle event generator)监控pod是否健康运行。它的首要功能如下:
- 从metamanager中接受和处理pod添加，删除和修改的消息。
- 掌控分离的worker queues以添加或删除pod。
- 掌控worker rountines以检查worker queues
- keeps separate cache for config map and secrets respectively。
- 定期清理孤儿pod。

# hadoop
## 安装
前置应用：
- sun java1.8(必须是1.8，否则会有各类错误)
```shell
wget http://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz
tar -xzvf hadoop-3.1.3.tar.gz
```
## 配置文件(./etc/hadoop下)
- core-site.xml(配置hadoop服务端口，文件路径等)
	```xml
	<property>
		<name>hadoop.tmp.dir</name>
		<value>file:/$your_hadoop_path/hdfs/tmp</value>
	<description>A base for other temporary directories.</description>
	</property>
		<property>
		<name>io.file.buffer.size</name>
	<value>131072</value>
	</property>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
	```
- hadoop-env\.sh
	```sh
	export JAVA_HOME=$your_java_home
	```
- hdfs-site.xml
	```xml
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:/home/hadoop/hadoop-2.7.3/hdfs/name</value>
		<final>true</final>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:/home/hadoop/hadoop-2.7.3/hdfs/data</value>
		<final>true</final>
	</property>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>master:9001</value>
	</property>
	<property>
		<name>dfs.webhdfs.enabled</name>
		<value>true</value>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>false</value>
	</property>
	```
- mapred-site.xml
	```xml
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
	```
- yarn-env.sh
	```sh
	export JAVA_HOME=$your_java_home
	```
- yarn-site.xml(yarn的一些服务端口配置)
	```xml
	<property>
		<name>yarn.resourcemanager.address</name>
		<value>master:18040</value>
	</property>
	<property>
		<name>yarn.resourcemanager.scheduler.address</name>
		<value>master:18030</value>
	</property>
	<property>
		<name>yarn.resourcemanager.webapp.address</name>
		<value>master:18088</value>
	</property>
	<property>
		<name>yarn.resourcemanager.resource-tracker.address</name>
		<value>master:18025</value>
	</property>
	<property>
		<name>yarn.resourcemanager.admin.address</name>
		<value>master:18141</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
	<property>
		 <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
		 <value>org.apache.hadoop.mapred.ShuffleHandler</value>
	</property>
	```
- workers
	```
	localhost删掉
	node1
	```
## 一些坑点
- java版本必须1.8
- root用户下配置,而不是新建一个hadoop用户
	```sh
	./sbin/start-yarn.sh
	./sbin/stop-yarn.sh
	YARN_RESOURCEMANAGER_USER=root
	HDFS_DATANODE_SECURE_USER=yarn
	YARN_NODEMANAGER_USER=root

	./sbin/start-dfs.sh
	./sbin/stop-dfs.sh
	HDFS_DATANODE_USER=root
	HDFS_DATANODE_SECURE_USER=hdfs
	HDFS_NAMENODE_USER=root
	HDFS_SECONDARYNAMENODE_USER=root
	```
- 重启脚本
	```sh
	#!/usr/bin/env sh
	./sbin/stop-all.sh
	rm -rf hdfs.sh
	rm -rf logs/*
	ssh root@node1 "rm -rf /root/hadoop-3.1.3/hdfs"
	hadoop namenode -format
	./sbin/start-all.sh
	```
- 报错没有启动datanode，或启动了datanode`hdfs dfsadmin -report`仍显示0datanode，这是由于重启hadoop时没有删除上一次启动时的残留信息。若此时./hdfs目录下没有珍贵数据，删除这个文件夹即可。
- 一些命令
	```sh
	hdfs dfsadmin -report #查看集群状态
	./sbin/start-all.sh #启动集群
	./sbin/stop-all.sh #关闭集群
	hadoop namenode -format #格式化集群
	jps #查看当前节点下运行的各项服务
	```
- 正常集群下jps输出
	```
	master:
	Jps
	NameNode
	ResourceManager
	SecondaryNameNode

	node1:
	Jps
	NodeManager
	DataNode
	```
# mysql安装
```sh
wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm
yum -y install mysql57-community-release-el7-10.noarch.rpm
yum -y install mysql-community-server
systemctl start mysqld
grep "password" /var/log/mysqld.log
mysql -uroot -p
mysql> set global validate_password_policy=0;
mysql> set global validate_password_length=1;
mysql>  ALTER USER 'root'@'localhost' IDENTIFIED BY 'new password';
mysql> create user 'hive_user'@'%' identified by 'hive_pwd'; 这是Hive连接MySQL的用户名和密码，一会儿配置Hive时有用到。（要创建一个新的用户给hive使用，怕原有的用户元数据删除不干净或者有某些配置文件无法删除掉吧）
mysql> grant all privileges on *.* to 'hive_user'@'%' with grant option; 分配权限
mysql> flush privileges;
create user 'hive'@'%' identified by '123456';
grant all privileges on *.* to 'hive'@'%' with grant option;
flush privileges;
yum -y remove mysql57-community-release-el7-10.noarch
```
# hive
[hive安装](https://blog.csdn.net/u013412497/article/details/54914823)
```
CREATE EXTERNAL TABLE sogou_20111230(ts string, uid string, keyword string, rank int, order_num int, url string)
COMMENT 'This is the sogou search data of one day'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
location '~/sogou/sogou.10k.utf8'

CREATE EXTERNAL TABLE s500(ts string, uid string, keyword string, rank int, order_num int, url string)
COMMENT 'This is the sogou search data of one day'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
location '/root/sogou/sogou.500w.utf8'

CREATE EXTERNAL TABLE ss500(day string,hour string, uid string, keyword string, rank int, order_num int, url string)
COMMENT 'This is the sogou search data of one day'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE
location '/root/sogou/ss500'

load data local inpath '/root/sogou/sogou.500w.utf' into table s500;
load data local inpath '/root/sogou/ss500' into table ss500;

select keyword,count(*) as cnt from s500 group by keyword order by cnt desc limit 50;

select sum(a.cnt)/count(a.uid) from (select uid,count(*) as cnt from s500 group by uid) a;

CREATE external table  pronword(words string);
select SUM(IF(uids.cnt=1,1,0)),SUM(IF(uids.cnt=2,1,0)),SUM(IF(uids.cnt=3,1,0)),SUM(IF(uids.cnt>3,1,0))from (select uid,count(*) as cnt from s500 group by uid) uids;

select * from s500 where s500.keyword in (select * from pronword);

select hour,count(*) from ss500 group by hour order by hour;
select hour,count(*) from ss500 where keyword like '%人体艺术%' group by hour order by hour;
select hour,count(*) from ss500 where keyword like '%儿子与母亲不正常关系%' group by hour order by hour;
select hour,count(*) from ss500 where keyword like '%快播%' group by hour order by hour;

select hour,count(*) from ss500 where keyword like '%新疆暴徒%' group by hour order by hour;
```
	
